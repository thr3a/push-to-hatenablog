---
Title: TensorFlowからGPUが認識できているかを2行コードで確認する
Category:
- python
Date: 2018-01-13T14:11:05+09:00
URL: https://blog.turai.work/entry/20180113/1515820265
EditURL: https://blog.hatena.ne.jp/thr3a/thr3a.hatenablog.com/atom/entry/8599973812336863797
---

GPUのドライバ入れた！CUDAもOK！けどTensorFlowでちゃんと使えてるかわからん！ってときの確認用

# 環境

- Python 3.6.2 Anaconda
- Ubuntu


# 方法

```python
from tensorflow.python.client import device_lib
device_lib.list_local_devices()
```

を実行してdevice_type:GPUのやつがあれば認識してる。

実際にやってみると、

```
$ python
>>> from tensorflow.python.client import device_lib
  return f(*args, **kwds)
>>> device_lib.list_local_devices()
2018-01-12 20:09:07.451631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-12 20:09:07.452057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:04.0
totalMemory: 11.17GiB freeMemory: 11.10GiB
2018-01-12 20:09:07.452093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 2319180638018740093
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 11324325888
locality {
  bus_id: 1
}
incarnation: 13854674477442207273
physical_device_desc: "device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7"
]
```

って感じ

認識できてない場合（GPUとTensorFlowとの連携がうまく行ってない場合）だと以下のようにCPUだけになる。

```
>>> device_lib.list_local_devices()
[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 2286130473433412332
]
```

ちなみに自分の場合は `pip install tensorflow-gpu` が足りなかった
